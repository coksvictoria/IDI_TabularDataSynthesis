{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW31nfYrngcu"
      },
      "source": [
        "##load library and def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-3A7Lxtb--I",
        "outputId": "bab47110-d3ce-4743-cb32-b676e65d73e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Load library\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "from typing import Union, List, Optional,Tuple, Dict, Callable,Any\n",
        "\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = \"{:,.2f}\".format\n",
        "\n",
        "import math\n",
        "from scipy import stats\n",
        "from scipy.special import kl_div\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "\n",
        "import time\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore') ##filter out LR warning\n",
        "\n",
        "import itertools\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import roc_auc_score,average_precision_score,accuracy_score,precision_score,pairwise_distances,recall_score,silhouette_score,f1_score,jaccard_score,matthews_corrcoef,pairwise\n",
        "\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder,MinMaxScaler\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier,KDTree,DistanceMetric,BallTree,KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier,IsolationForest,ExtraTreesClassifier,ExtraTreesRegressor\n",
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.decomposition import PCA,KernelPCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.svm import SVC,SVR\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jai97Ok-WCx2"
      },
      "source": [
        "##Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A5U_AAqEdMFk"
      },
      "outputs": [],
      "source": [
        "def adjusted_classes(y_scores, t):\n",
        "    \"\"\"\n",
        "    This function adjusts class predictions based on the prediction threshold (t).\n",
        "    Will only work for binary classification problems.\n",
        "    \"\"\"\n",
        "    return [1 if y >= t else 0 for y in y_scores]\n",
        "\n",
        "def data_preprocess(reald,faked,cat_cols,onehot=False):\n",
        "\n",
        "  real=reald.copy().reset_index(drop=True)\n",
        "  fake=faked.copy().reset_index(drop=True)\n",
        "\n",
        "  ss=MinMaxScaler()\n",
        "  ohe = OneHotEncoder(sparse=False,handle_unknown=\"ignore\")\n",
        "\n",
        "  col_names=real.columns.to_list()\n",
        "  n_col=[s for s in col_names if s not in cat_cols]\n",
        "\n",
        "  real_scaled=pd.DataFrame(ss.fit_transform(real[n_col]), index=None,columns=n_col)\n",
        "  fake_scaled=pd.DataFrame(ss.transform(fake[n_col]), index=None,columns=n_col)\n",
        "\n",
        "  if onehot:\n",
        "\n",
        "    b_cols=[]\n",
        "    for i in real.columns:\n",
        "      if real[i].nunique()==2:\n",
        "        b_cols.append(i)\n",
        "\n",
        "    cat_cols=[i for i in cat_cols if i not in b_cols]\n",
        "\n",
        "    #One-hot-encode the categorical columns.\n",
        "    #Unfortunately outputs an array instead of dataframe.\n",
        "    ohe_real = ohe.fit_transform(real[cat_cols])\n",
        "    ohe_fake = ohe.transform(fake[cat_cols])\n",
        "    #Convert it to df\n",
        "    real_encoded = pd.DataFrame(ohe_real, index=None,columns=ohe.get_feature_names_out())\n",
        "    fake_encoded = pd.DataFrame(ohe_fake, index=None,columns=ohe.get_feature_names_out())\n",
        "\n",
        "    real_processed=pd.concat([real_scaled, real_encoded,real[b_cols]], axis=1)\n",
        "    fake_processed=pd.concat([fake_scaled, fake_encoded,fake[b_cols]], axis=1)\n",
        "  else:\n",
        "    real_processed=pd.concat([real_scaled, real[cat_cols]], axis=1)[col_names]\n",
        "    fake_processed=pd.concat([fake_scaled, fake[cat_cols]], axis=1)[col_names]\n",
        "\n",
        "  return real_processed,fake_processed\n",
        "\n",
        "  # reals,fakes=data_preprocess(real,fake,c_col)\n",
        "# reale,fakee=data_preprocess(real,fake,c_col,onehot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e69fbA4_y0PG"
      },
      "source": [
        "##Synthetic Data Evaluation - Utility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX3eV47VFEmu"
      },
      "source": [
        "###Univariate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y4gUXepycKt"
      },
      "source": [
        "####Descriptive Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MjXI1ipsyfCL"
      },
      "outputs": [],
      "source": [
        "def summary_stats(real,fake,c_col,n_col):\n",
        "  n_stats=real[n_col].describe().T-fake[n_col].describe().T\n",
        "  c_stats=real[c_col].astype('object').describe().T-fake[c_col].astype('object').describe().T\n",
        "  return n_stats,c_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfb4hOTUIDhb"
      },
      "source": [
        "####Univariate test statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fq3lbapmFHfc"
      },
      "outputs": [],
      "source": [
        "##compare continous columns between real and fake\n",
        "def kolmogorov_smirnov_test(col_name, real_col, fake_col):\n",
        "    statistic, p_value = stats.ks_2samp(real_col, fake_col)\n",
        "    equality = 'identical' if p_value > 0.01 else 'different'\n",
        "    return {'col_name': col_name, 'ks_statistic': statistic, 'ks_p-value': p_value, 'equal distribution': equality}\n",
        "\n",
        "def mood_test(col_name, real_col, fake_col):\n",
        "    statistic, p_value,m,t = stats.median_test(real_col, fake_col,ties=\"below\",nan_policy=\"omit\")\n",
        "    equality = 'identical' if p_value > 0.01 else 'different'\n",
        "    return {'col_name': col_name, 'md_statistic': statistic, 'md_p-value': p_value, 'equal median': equality}\n",
        "\n",
        "def mannwhitneyu(col_name, real_col, fake_col):\n",
        "    statistic, p_value = stats.mannwhitneyu(real_col, fake_col)\n",
        "    equality = 'identical' if p_value > 0.01 else 'different'\n",
        "    return {'col_name': col_name, 'mw_statistic': statistic, 'mw_p-value': p_value, 'equal mean': equality}\n",
        "\n",
        "def levene_test(col_name, real_col, fake_col):\n",
        "    statistic, p_value = stats.levene(real_col, fake_col)\n",
        "    equality = 'identical' if p_value > 0.01 else 'different'\n",
        "    return {'col_name': col_name, 'le_statistic': statistic, 'le_p-value': p_value, 'equal variance': equality}\n",
        "\n",
        "\n",
        "def num_statistics_df(real: pd.DataFrame, fake: pd.DataFrame, stats_func: Callable, numerical_columns=None) -> List[Dict[str, Any]]:\n",
        "  assert real.columns.tolist() == fake.columns.tolist(), f'Colums are not identical between `real` and `fake`. '\n",
        "\n",
        "  real_iter = real[numerical_columns].items()\n",
        "  fake_iter = fake[numerical_columns].items()\n",
        "  distances = Parallel(n_jobs=-1)(delayed(stats_func) (colname, real_col, fake_col) for (colname, real_col), (_, fake_col) in zip(real_iter, fake_iter))\n",
        "  distances_df = pd.DataFrame(distances).set_index('col_name')\n",
        "  distances_df.loc['mean']  = distances_df.mean(numeric_only=True)\n",
        "  return distances_df\n",
        "\n",
        "def get_frequencies(real, synthetic,percent=True):\n",
        "    \"\"\"Get percentual frequencies for each possible real categorical value.\n",
        "    Given two iterators containing categorical data, this transforms it into\n",
        "    observed/expected frequencies which can be used for statistical tests. It\n",
        "    adds a regularization term to handle cases where the synthetic data contains\n",
        "    values that don't exist in the real data.\n",
        "    Args:\n",
        "        real (list):\n",
        "            A list of hashable objects.\n",
        "        synthetic (list):\n",
        "            A list of hashable objects.\n",
        "    Yields:\n",
        "        tuble[list, list]:\n",
        "            The observed and expected frequencies (as a percent).\n",
        "    \"\"\"\n",
        "    f_obs, f_exp = [], []\n",
        "    real, synthetic = Counter(real), Counter(synthetic)\n",
        "    for value in synthetic:\n",
        "        if value not in real:\n",
        "            warnings.warn(f'Unexpected value {value} in synthetic data.')\n",
        "            real[value] += 1e-6  # Regularization to prevent NaN.\n",
        "\n",
        "    if percent:\n",
        "      for value in real:\n",
        "          f_obs.append(synthetic[value] / sum(synthetic.values()))  # noqa: PD011\n",
        "          f_exp.append(real[value] / sum(real.values()))  # noqa: PD011\n",
        "    else:\n",
        "      for value in real:\n",
        "          f_obs.append(synthetic[value])  # noqa: PD011\n",
        "          f_exp.append(real[value])  # noqa: PD011\n",
        "\n",
        "    return f_obs, f_exp\n",
        "\n",
        "##for categorical variables\n",
        "def chisquare_test(col_name, real_col, fake_col):\n",
        "  f_obs, f_exp = get_frequencies(real_col, fake_col)\n",
        "  statistic, p_value = stats.chisquare(f_obs, f_exp)\n",
        "  equality = 'identical' if p_value > 0.05 else 'different'\n",
        "  return {'col_name': col_name, 'ct_statistic': statistic, 'ct_p-value': p_value, 'equal frequency': equality}\n",
        "\n",
        "def cat_statistics_df(real: pd.DataFrame, fake: pd.DataFrame, stats_func: Callable, categorical_columns: List) -> pd.DataFrame:\n",
        "    assert real.columns.tolist() == fake.columns.tolist(), f'Colums are not identical between `real` and `fake`. '\n",
        "    real_iter = real[categorical_columns].items()\n",
        "    fake_iter = fake[categorical_columns].items()\n",
        "    distances = Parallel(n_jobs=-1)(delayed(stats_func) (colname, real_col, fake_col) for (colname, real_col), (_, fake_col) in zip(real_iter, fake_iter))\n",
        "    distances_df = pd.DataFrame(distances).set_index('col_name')\n",
        "    distances_df.loc['mean']  = distances_df.mean(numeric_only=True)\n",
        "    return distances_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl_svleXIJKg"
      },
      "source": [
        "####Univariate distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "neJvqHCyILe3"
      },
      "outputs": [],
      "source": [
        "def get_frequency(X_gt: pd.DataFrame, X_synth: pd.DataFrame, n_histogram_bins: int = 10) -> dict:\n",
        "    \"\"\"Get percentual frequencies for each possible real categorical value.\n",
        "\n",
        "    Returns:\n",
        "        The observed and expected frequencies (as a percent).\n",
        "    \"\"\"\n",
        "    res = {}\n",
        "    for col in X_gt.columns:\n",
        "        local_bins = min(n_histogram_bins, len(X_gt[col].unique()))\n",
        "\n",
        "        if len(X_gt[col].unique()) < n_histogram_bins:  # categorical\n",
        "            gt = (X_gt[col].value_counts() / len(X_gt)).to_dict()\n",
        "            synth = (X_synth[col].value_counts() / len(X_synth)).to_dict()\n",
        "        else:\n",
        "            gt_vals, bins = np.histogram(X_gt[col], bins=local_bins)\n",
        "            synth_vals, _ = np.histogram(X_synth[col], bins=bins)\n",
        "            gt = {k: v / (sum(gt_vals) + 1e-8) for k, v in zip(bins, gt_vals)}\n",
        "            synth = {k: v / (sum(synth_vals) + 1e-8) for k, v in zip(bins, synth_vals)}\n",
        "\n",
        "        for val in gt:\n",
        "            if val not in synth or synth[val] == 0:\n",
        "                synth[val] = 1e-11\n",
        "        for val in synth:\n",
        "            if val not in gt or gt[val] == 0:\n",
        "                gt[val] = 1e-11\n",
        "\n",
        "        if gt.keys() != synth.keys():\n",
        "            raise ValueError(f\"Invalid features. {gt.keys()}. syn = {synth.keys()}\")\n",
        "        res[col] = (list(gt.values()), list(synth.values()))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def kl_divergence(colname,x,y):\n",
        "    relative_entropy=np.sum(kl_div(x,y))\n",
        "    return {'col_name': colname, 'kl_divergence':relative_entropy}\n",
        "\n",
        "def js_divergence(colname,x,y):\n",
        "    js=distance.jensenshannon(x,y)\n",
        "    return {'col_name': colname, 'js_distance': js}\n",
        "\n",
        "def num_divergence_df(real: pd.DataFrame, fake: pd.DataFrame,stats_func: Callable, numerical_columns=None):\n",
        "\n",
        "    freqs=get_frequency(real,fake)\n",
        "    res = {}\n",
        "    for col in real.columns:\n",
        "        real_freq, fake_freq = freqs[col]\n",
        "        res[col]=stats_func(col,real_freq,fake_freq)\n",
        "    distances_df=pd.DataFrame(res).T.set_index('col_name')\n",
        "    distances_df.loc['mean']=distances_df.mean()\n",
        "    return distances_df\n",
        "\n",
        "def em_distance(colname,x,y):\n",
        "    em=stats.wasserstein_distance(x,y)\n",
        "    return {'col_name': colname, 'em_distance': em}\n",
        "\n",
        "def num_distance_df(real: pd.DataFrame, fake: pd.DataFrame, stats_func: Callable, numerical_columns=None) -> List[Dict[str, Any]]:\n",
        "    assert real.columns.tolist() == fake.columns.tolist(), f'Colums are not identical between `real` and `fake`. '\n",
        "    if numerical_columns is None:\n",
        "      numerical_columns=real.columns.tolist()\n",
        "    real_iter = real[numerical_columns].items()\n",
        "    fake_iter = fake[numerical_columns].items()\n",
        "    distances = Parallel(n_jobs=-1)(delayed(stats_func) (colname, real_col.values, fake_col.values) for (colname, real_col), (_, fake_col) in zip(real_iter, fake_iter))\n",
        "    distances_df = pd.DataFrame(distances).set_index('col_name')\n",
        "    distances_df.loc['mean']=distances_df.mean()\n",
        "    return distances_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZGexVs2y-5f"
      },
      "source": [
        "###Bivariate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kp5w1CcfzArM"
      },
      "outputs": [],
      "source": [
        "#####Nom to Nom (Cat to Cat) stats############\n",
        "def conditional_entropy(x,y):\n",
        "    # entropy of x given y\n",
        "    y_counter = Counter(y)\n",
        "    xy_counter = Counter(list(zip(x,y)))\n",
        "    total_occurrences = sum(y_counter.values())\n",
        "    entropy = 0\n",
        "    for xy in xy_counter.keys():\n",
        "        p_xy = xy_counter[xy] / total_occurrences\n",
        "        p_y = y_counter[xy[1]] / total_occurrences\n",
        "        entropy += p_xy * math.log(p_y/p_xy)\n",
        "    return entropy\n",
        "\n",
        "def theils_u(x,y):\n",
        "    \"\"\"\n",
        "    also referred to as the Uncertainty Coefficient, is based on the conditional entropy between x and y —\n",
        "    given the value of x, how many possible states does y have, and how often do they occur. Just like Cramer’s V, the output value is on the range of [0,1]\n",
        "    asymmetric, meaning U(x,y)≠U(y,x) (while V(x,y)=V(y,x), where V is Cramer’s V)\n",
        "    \"\"\"\n",
        "    s_xy = conditional_entropy(x,y)\n",
        "    x_counter = Counter(x)\n",
        "    total_occurrences = sum(x_counter.values())\n",
        "    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n",
        "    s_x = stats.entropy(p_x)\n",
        "    if s_x == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return (s_x - s_xy) / s_x\n",
        "\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"\n",
        "    Calculates Cramer's V statistic for categorical-categorical association.\n",
        "    This is a symmetric coefficient: V(x,y) = V(y,x)\n",
        "    Parameters:\n",
        "    -----------\n",
        "    x : list / NumPy ndarray / Pandas Series  A sequence of categorical measurements\n",
        "    y : list / NumPy ndarray / Pandas Series  A sequence of categorical measurements\n",
        "    bias_correction : Boolean, default = True   Use bias correction from Bergsma and Wicher,Journal of the Korean Statistical Society 42 (2013): 323-328.\n",
        "    Returns:\n",
        "    --------\n",
        "    float in the range of [0,1]\n",
        "    \"\"\"\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    v = np.sqrt(phi2 / min(k - 1, r - 1))\n",
        "\n",
        "    if -0.0001 <= v < 0.0001 or  1. - 0.0001 < v <= 1. + 0.0001:\n",
        "        rounded_v = 0. if v < 0 else 1.\n",
        "        # warnings.warn(f'Rounded V = {v} to {rounded_v}. This is probably due to floating point precision issues.',   RuntimeWarning)\n",
        "        return rounded_v\n",
        "    else:\n",
        "        return v\n",
        "\n",
        "\n",
        "#####Nom to Num stats############\n",
        "def correlation_ratio(categories,measurements):\n",
        "    \"\"\"\n",
        "    Calculates the Correlation Ratio (sometimes marked by the greek letter Eta)\n",
        "    for categorical-continuous association. Answers the question - given a continuous value of a measurement, is it\n",
        "    possible to know which category is it associated with?Value is in the range [0,1], where 0 means a category cannot be determined\n",
        "    by a continuous measurement, and 1 means a category can be determined with absolute certainty.\n",
        "    Wikipedia: https://en.wikipedia.org/wiki/Correlation_ratio\n",
        "    Parameters:\n",
        "    -----------\n",
        "    categories : list / NumPy ndarray / Pandas Series\n",
        "        A sequence of categorical measurements\n",
        "    measurements : list / NumPy ndarray / Pandas Series\n",
        "        A sequence of continuous measurements\n",
        "    Returns:\n",
        "    --------\n",
        "    float in the range of [0,1]\n",
        "    \"\"\"\n",
        "    fcat, _ = pd.factorize(categories)\n",
        "    cat_num = np.max(fcat) + 1\n",
        "    y_avg_array = np.zeros(cat_num)\n",
        "    n_array = np.zeros(cat_num)\n",
        "    for i in range(0, cat_num):\n",
        "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
        "        n_array[i] = len(cat_measures)\n",
        "        y_avg_array[i] = np.average(cat_measures)\n",
        "    y_total_avg = np.sum(np.multiply(y_avg_array, n_array)) / np.sum(n_array)\n",
        "    numerator = np.sum(\n",
        "        np.multiply(n_array, np.power(np.subtract(y_avg_array, y_total_avg),\n",
        "                                      2)))\n",
        "    denominator = np.sum(np.power(np.subtract(measurements, y_total_avg), 2))\n",
        "    if numerator == 0:\n",
        "        return 0.\n",
        "    else:\n",
        "        eta = np.sqrt(numerator / denominator)\n",
        "        if 1. < eta <= 1.+0.0000:\n",
        "            warnings.warn(f'Rounded eta = {eta} to 1. This is probably due to floating point precision issues.',\n",
        "                          RuntimeWarning)\n",
        "            return 1.\n",
        "        else:\n",
        "            return eta\n",
        "\n",
        "def column_associations(real: pd.DataFrame, c_col : List, theil_u=False) -> pd.DataFrame:\n",
        "\n",
        "  corr=pd.DataFrame(index=real.columns,columns=real.columns)\n",
        "\n",
        "  b_col=[]\n",
        "  m_col=[]\n",
        "  for i in c_col:\n",
        "    unique_values = pd.unique(real[i])\n",
        "    if len(unique_values) == 2:\n",
        "      b_col.append(i)\n",
        "    else:\n",
        "      m_col.append(i)\n",
        "\n",
        "  for i,ac in enumerate(corr):\n",
        "    for j, bc in enumerate(corr):\n",
        "      if i > j:\n",
        "        continue\n",
        "\n",
        "      if ac in c_col and bc in c_col:\n",
        "        if ac in b_col and bc in b_col:\n",
        "          c=matthews_corrcoef(real[ac].values,real[bc].values)\n",
        "        else:\n",
        "          if theil_u:\n",
        "            c= theils_u(real[ac].values,real[bc].values)\n",
        "          else:\n",
        "            c=cramers_v(real[ac].values,real[bc].values)\n",
        "      else:\n",
        "        if ac in b_col or bc in b_col:\n",
        "          c,_=stats.pointbiserialr(real[ac].values,real[bc].values)\n",
        "        else:\n",
        "          if ac in c_col or bc in c_col:\n",
        "            c=correlation_ratio(real[ac].values,real[bc].values)\n",
        "          else:\n",
        "            c, _ = stats.pearsonr(real[ac].sort_values(), real[bc].sort_values())\n",
        "      corr.loc[ac,bc]=corr.loc[bc,ac]=c\n",
        "  return corr\n",
        "\n",
        "def bivariate_test(real,fake,c_col):\n",
        "  real_corr=column_associations(real,c_col,True)\n",
        "  fake_corr=column_associations(fake,c_col,True)\n",
        "  statistics,p=stats.ks_2samp(real_corr.to_numpy().flatten(),fake_corr.to_numpy().flatten())\n",
        "\n",
        "  return round(p,4),real_corr,fake_corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj6ik5FuFj0z"
      },
      "source": [
        "###Multivariate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mue5XnkSFm7L"
      },
      "source": [
        "####PCA correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EGqgJ73lFpzr"
      },
      "outputs": [],
      "source": [
        "def pca_correlation(real, fake):\n",
        "    \"\"\"\n",
        "    Calculate the relation between PCA explained variance values. Due to some very large numbers, in recent implementation the MAPE(log) is used instead of\n",
        "    regressions like Pearson's r.\n",
        "\n",
        "    \"\"\"\n",
        "    real=real.copy()\n",
        "    fake=fake.copy()\n",
        "    n_components=real.shape[1]#//4\n",
        "    pca_r = PCA(n_components=n_components)\n",
        "    pca_f = PCA(n_components=n_components)\n",
        "\n",
        "    pca_r.fit(real)\n",
        "    pca_f.fit(fake)\n",
        "\n",
        "    results = pd.DataFrame({'real': pca_r.explained_variance_, 'fake': pca_f.explained_variance_})\n",
        "    print(f'\\n######################################PCA Correlation####################################')\n",
        "    print(f'\\nTop {n_components} PCA components:')\n",
        "    print(results.to_string())\n",
        "\n",
        "    statistics,p=stats.ks_2samp(pca_r.explained_variance_, pca_f.explained_variance_)\n",
        "\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLnLA8U-MakY"
      },
      "source": [
        "####Maximum Mean Discrepancy (MMD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gKRWd8GtMg09"
      },
      "outputs": [],
      "source": [
        "def mmd_kernel(X,Y,kernel='rbf',gamma=1,degree=2,coef0=0):\n",
        "  \"\"\"MMD using linear/rbf/polynomial kernel (i.e., k(x,y) = (gamma <X, Y> + coef0)^degree)\n",
        "  Arguments:\n",
        "      X {[n_sample1, dim]} -- [X matrix]\n",
        "      Y {[n_sample2, dim]} -- [Y matrix]\n",
        "  Keyword Arguments:\n",
        "      gamma {int} -- [gamma] (default: {1})\n",
        "      degree {int} -- [degree] (default: {2})\n",
        "      coef0 {int} -- [constant item] (default: {0})\n",
        "  Returns:\n",
        "      [scalar] -- [MMD value]\n",
        "    \"\"\"\n",
        "  if kernel == 'linear':\n",
        "    delta = X.mean(0) - Y.mean(0)\n",
        "    score = delta.dot(delta.T)\n",
        "\n",
        "  elif kernel == 'rbf':\n",
        "    XX = pairwise.rbf_kernel(X, X, gamma)\n",
        "    YY = pairwise.rbf_kernel(Y, Y, gamma)\n",
        "    XY = pairwise.rbf_kernel(X, Y, gamma);\n",
        "    score=XX.mean() + YY.mean() - 2 * XY.mean()\n",
        "\n",
        "\n",
        "  elif kernel == 'polynomial':\n",
        "    XX = pairwise.polynomial_kernel(X, X, degree, gamma, coef0)\n",
        "    YY = pairwise.polynomial_kernel(Y, Y, degree, gamma, coef0)\n",
        "    XY = pairwise.polynomial_kernel(X, Y, degree, gamma, coef0)\n",
        "    score = XX.mean() + YY.mean() - 2 * XY.mean()\n",
        "  else:\n",
        "    raise ValueError(f\"Unsupported kernel {kernel}\")\n",
        "\n",
        "  return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "185gwSk6pyv9"
      },
      "source": [
        "####ML Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW3ME6HZp00o"
      },
      "outputs": [],
      "source": [
        "def ml_evaluation(real, fake, df_test, target_col: str, target_type: str = 'class' ,data_strategy_flag=False, verbose: bool =True):\n",
        "\n",
        "    df_test=df_test.copy()\n",
        "\n",
        "    real=real.copy()\n",
        "    fake=fake.copy()\n",
        "\n",
        "    real_x,real_y = real.drop([target_col], axis=1),real[target_col]\n",
        "    fake_x,fake_y = fake.drop([target_col], axis=1),fake[target_col]\n",
        "    X_test,y_test = df_test.drop([target_col], axis=1),df_test[target_col]\n",
        "\n",
        "    if target_type == 'regr':\n",
        "\n",
        "      estimators = [\n",
        "          KNeighborsRegressor(),\n",
        "          SVR(kernel = 'rbf'),\n",
        "          lgb.LGBMRegressor(n_estimators=100,random_state=1),\n",
        "          RandomForestRegressor(n_estimators=100, random_state=1),\n",
        "          Lasso(random_state=1),\n",
        "          Ridge(alpha=1.0, random_state=1),\n",
        "          ElasticNet(random_state=1)\n",
        "      ]\n",
        "\n",
        "      estimators_names = ['KNN','SVC','LGBM','RF','LS','RD','EN']\n",
        "\n",
        "    elif target_type == 'class':\n",
        "      estimators = [\n",
        "          KNeighborsClassifier(),\n",
        "          lgb.LGBMClassifier(n_estimators=100,random_state=1),\n",
        "          RandomForestClassifier(n_estimators=100, random_state=1),\n",
        "          LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=500, random_state=1),\n",
        "          DecisionTreeClassifier(random_state=1),\n",
        "          MLPClassifier([50, 50], solver='adam', activation='relu', learning_rate='adaptive', random_state=1)\n",
        "      ]\n",
        "\n",
        "      estimators_names = ['KNN','LGBM','RF','LR','DT','MLP']\n",
        "    else:\n",
        "        raise ValueError(f'target_type must be \\'regr\\' or \\'class\\'')\n",
        "\n",
        "    zipped_estimators= zip(estimators_names,estimators)\n",
        "\n",
        "    R=[]\n",
        "\n",
        "    if target_type == 'class':\n",
        "        for est_name,est in zipped_estimators:\n",
        "          start = time.time()\n",
        "          print(est_name)\n",
        "\n",
        "          if(len(np.unique(fake_y)))==1:\n",
        "            R.append(['data synthesis',est_name,0,0,0,0,0,0])\n",
        "          else:\n",
        "            model= est\n",
        "            model.fit(fake_x,fake_y)\n",
        "            y_test_probas = model.predict_proba(X_test)[:,1]\n",
        "            y_test_predicted = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_test_predicted)\n",
        "            pre = precision_score(y_test, y_test_predicted)\n",
        "            rec = recall_score(y_test, y_test_predicted)\n",
        "            f1=f1_score(y_test, y_test_predicted)\n",
        "            aucpr = average_precision_score(y_test, y_test_probas)\n",
        "            aucroc = roc_auc_score(y_test, y_test_probas)\n",
        "\n",
        "            R.append(['data synthesis',est_name,acc,pre,rec,f1,aucpr,aucroc])\n",
        "\n",
        "\n",
        "          if data_strategy_flag:\n",
        "            real_fake=pd.concat([real,fake])\n",
        "            real_fake_pos=pd.concat([real,fake[fake[target_col]==1]])\n",
        "            real_fake_x,real_fake_y=real_fake.drop([target_col], axis=1),real_fake[target_col]\n",
        "            real_fake_pos_x,real_fake_pos_y=real_fake_pos.drop([target_col], axis=1),real_fake_pos[target_col]\n",
        "            model= est\n",
        "            model.fit(real_fake_pos_x,real_fake_pos_y)\n",
        "            y_test_probas = model.predict_proba(X_test)[:,1]\n",
        "            y_test_predicted = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_test_predicted)\n",
        "            pre = precision_score(y_test, y_test_predicted)\n",
        "            rec = recall_score(y_test, y_test_predicted)\n",
        "            f1=f1_score(y_test, y_test_predicted)\n",
        "            aucpr = average_precision_score(y_test, y_test_probas)\n",
        "            aucroc = roc_auc_score(y_test, y_test_probas)\n",
        "            t = round(time.time() - start,2)\n",
        "            R.append(['data balancing',est_name,acc,pre,rec,f1,aucpr,aucroc])\n",
        "\n",
        "            model= est\n",
        "            model.fit(real_fake_x,real_fake_y)\n",
        "            y_test_probas = model.predict_proba(X_test)[:,1]\n",
        "            y_test_predicted = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_test_predicted)\n",
        "            pre = precision_score(y_test, y_test_predicted)\n",
        "            rec = recall_score(y_test, y_test_predicted)\n",
        "            f1=f1_score(y_test, y_test_predicted)\n",
        "            aucpr = average_precision_score(y_test, y_test_probas)\n",
        "            aucroc = roc_auc_score(y_test, y_test_probas)\n",
        "\n",
        "            R.append(['data augmentation',est_name,acc,pre,rec,f1,aucpr,aucroc])\n",
        "\n",
        "          t = round(time.time() - start,2)\n",
        "          print(f'{est_name} takes {t} seconds to run')\n",
        "    dfResults = pd.DataFrame(data=R, columns=['data_strategy','clf_name','acc','pre','rec','f1','aucpr','aucroc']).sort_values(['data_strategy'])\n",
        "\n",
        "    return dfResults"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCtBxQGhrXr9"
      },
      "source": [
        "####Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X0h3B6p4raQc"
      },
      "outputs": [],
      "source": [
        "def feature_importance_comparison(X, Y, estimator,target_col,method_names=None):\n",
        "    num_methods = len(X)\n",
        "\n",
        "    importances = []\n",
        "    stds = None\n",
        "\n",
        "    fi_dict={}\n",
        "    for i in range(num_methods):\n",
        "        estimator=estimator.fit(X[i], Y[i])\n",
        "        fi_dict[method_names[i]]=estimator.feature_importances_\n",
        "        importances.append(estimator.feature_importances_)\n",
        "\n",
        "        if method_names is not None and i>0:\n",
        "            print(f'\\nCorrelation of importances : {method_names[i]}')\n",
        "            print(np.corrcoef(importances[0],importances[i])[0,1])\n",
        "\n",
        "    if method_names is not None:\n",
        "        bar_comparison(importances,stds, labels=method_names, tick_names = X[0].columns, save_name = 'all_feat_importance',target_col=target_col)\n",
        "\n",
        "    return importances\n",
        "\n",
        "def bar_comparison(vectors, std=None, labels=None, tick_names=None, save_name = None, target_col=None,max_length = 5):\n",
        "\n",
        "    num_bars = len(vectors)\n",
        "    vector = vectors[0]\n",
        "    indices = np.argsort(vector)[::-1]\n",
        "    indices = indices[:max_length]\n",
        "    fig, ax = plt.subplots(figsize=(12,4))\n",
        "    tot_bar_width = 0.7\n",
        "    width = tot_bar_width/num_bars\n",
        "    x = np.arange(len(indices))\n",
        "\n",
        "    if tick_names is None:\n",
        "        tick_names = range(len(vector))\n",
        "\n",
        "    if labels is None:\n",
        "        labels = ['Original', 'Synthetic', 'Transfer']\n",
        "\n",
        "    for i, vec in enumerate(vectors):\n",
        "        xbar = x - tot_bar_width/2 + (i+1/2)*width\n",
        "        if std is not None:\n",
        "            ax.bar(xbar, vec[indices],  yerr=std[i][indices], width=width, label=labels[i])\n",
        "        else:\n",
        "            ax.bar(xbar, vec[indices], width=width, label=labels[i])\n",
        "\n",
        "    ax.set_ylim(bottom=0)\n",
        "    fig.tight_layout()\n",
        "    ticks = np.array(tick_names, dtype='object')\n",
        "    ticks = ticks[indices]\n",
        "    plt.xticks(x, ticks,fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.xlim([-1, len(indices)])\n",
        "    fig.savefig('feature_imp.pdf')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def feature_importance(real, fake_dfs, target_col: str, c_col: str,target_type: str = 'class' ,verbose: bool =True) -> float:\n",
        "\n",
        "    real_copy=real.copy()\n",
        "\n",
        "    real,_=data_preprocess(real_copy,real_copy,c_col,onehot=True)\n",
        "    display(real)\n",
        "    real_x = real.drop([target_col], axis=1)\n",
        "    real_y = real[target_col]\n",
        "\n",
        "    X, Y = [real_x], [real_y]\n",
        "\n",
        "    mothed_names=['Identity']\n",
        "    for n,fake in fake_dfs.items():\n",
        "      _,fake=data_preprocess(real_copy,fake,c_col,onehot=True)\n",
        "      fake_x = fake.drop([target_col], axis=1)\n",
        "      fake_y = fake[target_col]\n",
        "\n",
        "      X.append(fake_x)\n",
        "      Y.append(fake_y)\n",
        "      mothed_names.append(n)\n",
        "\n",
        "    if target_type == 'regr':\n",
        "        estimator=lgb.LGBMRegressor(n_estimators=100,random_state=1, importance_type='gain')\n",
        "    elif target_type == 'class':\n",
        "        estimator=lgb.LGBMClassifier(n_estimators=100,random_state=1, importance_type='gain')\n",
        "    else:\n",
        "        raise ValueError(f'target_type must be \\'regr\\' or \\'class\\'')\n",
        "\n",
        "    method_names=['Identity', 'Simple simulation', 'Distance-based models', 'Statistical models', 'DGMs']\n",
        "    feat_imp = feature_importance_comparison(X,Y,estimator=estimator,target_col=target_col,method_names=['Identity', 'Simple simulation', 'Distance-based models', 'Statistical models', 'DGMs'])#mothed_names)\n",
        "\n",
        "    return feat_imp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugy-_Lh-0258"
      },
      "source": [
        "###Visual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1uAZB1f09wC"
      },
      "source": [
        "####Univariate Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egz44rFe04or"
      },
      "outputs": [],
      "source": [
        "def univariate_plot(real,fake,n_col=None,filename=''):\n",
        "    real=real.copy()\n",
        "    fake=fake.copy()\n",
        "\n",
        "    if n_col is None:\n",
        "      n_col=real.columns.tolist()\n",
        "\n",
        "    real['dataset']='real'\n",
        "    fake['dataset']='fake'\n",
        "    df_rf=pd.concat([real,fake]).reset_index(drop=True)\n",
        "\n",
        "    n_var=len(n_col)\n",
        "\n",
        "    fig,axs= plt.subplots(nrows=n_var,ncols=2,figsize=(16,5*n_var))\n",
        "\n",
        "    idx=0\n",
        "    for i in n_col:\n",
        "      sns.histplot(x=i,hue='dataset',data=df_rf,alpha=0.5,ax=axs[idx][0])\n",
        "      ax2 = axs[idx][0].twinx()\n",
        "      sns.kdeplot(x=i,hue='dataset',data=df_rf,alpha=1,ax=ax2).set(title=\"Kernel Density Function\")\n",
        "      sns.histplot(x=i,hue='dataset',data=df_rf,alpha=0.5,bins=len(df_rf), stat=\"density\",element=\"step\"\n",
        "      , fill=False, cumulative=True, common_norm=False,ax=axs[idx][1]).set(title=\"Cumulative Distribution Function\")\n",
        "      axs[idx][1].set(ylabel=None)\n",
        "      idx+=1\n",
        "\n",
        "    if filename!='':\n",
        "      fig.savefig(filename)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def univariate_single_plot(real,fake,var_name,figsize=(16,5),filename=''):\n",
        "    real['dataset']='real'\n",
        "    fake['dataset']='fake'\n",
        "    df_rf=pd.concat([real,fake]).reset_index(drop=True)\n",
        "\n",
        "    _,axes= plt.subplots(1,2,figsize=figsize)\n",
        "    axes=plt.subplot(1,2,1)\n",
        "    sns.histplot(x=var_name,hue='dataset',data=df_rf,alpha=0.5)\n",
        "    axes.twinx()\n",
        "    sns.kdeplot(x=var_name,hue='dataset',data=df_rf,alpha=1).set(title=\"Kernel Density Function\",ylabel=None)\n",
        "    axes=plt.subplot(1,2,2)\n",
        "    sns.histplot(x=var_name,hue='dataset',data=df_rf,alpha=0.5,bins=len(df_rf), stat=\"density\",element=\"step\", fill=False, cumulative=True, common_norm=False).set(title=\"Cumulative Distribution Function\")\n",
        "\n",
        "    if filename!='':\n",
        "      fig.savefig(filename)\n",
        "\n",
        "    return plt\n",
        "\n",
        "# univariate_single_plot(real,fake,\"age\",(14,3))\n",
        "# univariate_plot(real,fake,n_col,filename='adult_univariate.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3fQpS-_4iO6"
      },
      "source": [
        "####Bivariate plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCHrV0CW4lIR"
      },
      "outputs": [],
      "source": [
        "def bivariate_plots(rc,fc, figsize= (10,15),filename=''):\n",
        "  fig, ((ax, ax2)) = plt.subplots(2, 1, figsize=figsize,constrained_layout = True)\n",
        "\n",
        "  rc_mask = np.tril(np.ones_like(rc, dtype=bool))\n",
        "  fc_mask = np.triu(np.ones_like(fc, dtype=bool))\n",
        "  sns.heatmap(rc.fillna(value=np.nan),mask=rc_mask,cmap='Greens', linewidths=0.5, yticklabels=True,ax=ax,annot=True,cbar=False,fmt='.2f',annot_kws={\"fontsize\":6})\n",
        "  sns.heatmap(fc.fillna(value=np.nan),mask=fc_mask,cmap='Blues', linewidths=0.5, yticklabels=True,ax=ax,annot=True,cbar=False,fmt='.2f',annot_kws={\"fontsize\":6})\n",
        "  # Rotate the tick labels and set their alignment.\n",
        "  plt.setp(ax.get_xticklabels(), rotation=45,ha=\"right\",rotation_mode=\"anchor\")\n",
        "  # ax.set_title(\"Correlations between real (green) Vs synthetic (blue) and its variance\", fontsize=12)\n",
        "\n",
        "  corr_diff=abs(rc-fc)\n",
        "  sns.heatmap(corr_diff.fillna(value=np.nan),mask=fc_mask,cmap='GnBu', xticklabels=False,vmin=0,vmax=1,linewidths=0.5, ax=ax2,annot=True,fmt='.2f',annot_kws={\"fontsize\":6})\n",
        "  # Rotate the tick labels and set their alignment.\n",
        "  # plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\n",
        "\n",
        "  # ax2.set_title(\"Difference of bivariate correlation between real and synthetic\", fontsize=12)\n",
        "\n",
        "  if filename!='':\n",
        "    fig.savefig(filename)\n",
        "    # fig.savefig(filename,tight_layout=True)\n",
        "\n",
        "# def bivariate_plots(rc,fc, figsize= (10,18),filename=''):\n",
        "#   fig, ((ax, ax2)) = plt.subplots(2, 1, figsize=figsize,constrained_layout = True)\n",
        "\n",
        "#   rc_mask = np.tril(np.ones_like(rc, dtype=bool))\n",
        "#   fc_mask = np.triu(np.ones_like(fc, dtype=bool))\n",
        "#   sns.heatmap(rc.fillna(value=np.nan),mask=rc_mask,cmap='Greens', linewidths=0.5, yticklabels=False,ax=ax,annot=True,cbar=False,fmt='.2f',annot_kws={\"fontsize\":6})\n",
        "#   sns.heatmap(fc.fillna(value=np.nan),mask=fc_mask,cmap='Blues', linewidths=0.5, yticklabels=False,ax=ax,annot=True,cbar=False,fmt='.2f',annot_kws={\"fontsize\":6})\n",
        "#   # Rotate the tick labels and set their alignment.\n",
        "#   plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\n",
        "#   ax.set_title(\"Bivariate correlation - real (green) Vs synthetic (blue)\", fontsize=12)\n",
        "\n",
        "#   corr_diff=abs(rc-fc)\n",
        "#   sns.heatmap(corr_diff.fillna(value=np.nan),mask=fc_mask,cmap='GnBu', vmin=0,vmax=1,linewidths=0.5, ax=ax2,annot=True,fmt='.2f',annot_kws={\"fontsize\":6})\n",
        "#   # Rotate the tick labels and set their alignment.\n",
        "#   plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\n",
        "#   ax2.set_title(\"Difference of bivariate correlation between real and synthetic\", fontsize=12)\n",
        "\n",
        "#   if filename!='':\n",
        "#     fig.savefig(filename)\n",
        "#     # fig.savefig(filename,tight_layout=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4inlOgs9gOQ"
      },
      "outputs": [],
      "source": [
        "def bivariate_plot(rc,fc, figsize= (14,10),cbar_flag=True,filename=''):\n",
        "  fig = plt.subplots(figsize=figsize)\n",
        "\n",
        "  rc_mask = np.tril(np.ones_like(rc, dtype=bool))\n",
        "  fc_mask = np.triu(np.ones_like(fc, dtype=bool))\n",
        "\n",
        "  corr_diff=round(abs(rc-fc),2)\n",
        "  hp=sns.heatmap(corr_diff.fillna(value=np.nan),mask=fc_mask,cmap='GnBu', vmin=0,vmax=1,linewidths=0.5,annot=True,fmt='.2f',annot_kws={\"fontsize\":6},cbar=cbar_flag)\n",
        "\n",
        "  # hp.set(title=\"Difference of bivariate correlations between real and synthetic\")\n",
        "  # # Rotate the tick labels and set their alignment.\n",
        "  plt.setp(hp.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\n",
        "\n",
        "  if filename!='':\n",
        "    fig.savefig(filename)\n",
        "    # fig.savefig(filename,tight_layout=True)\n",
        "\n",
        "# p,rc,fc=bivariate_test(reals,fakes,c_col)\n",
        "# bivariate_plots(rc,fc,filename='adult_smote.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD3VH1vgE2Ru"
      },
      "source": [
        "####Multi-Variate plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gpRHwjeaE5vG"
      },
      "outputs": [],
      "source": [
        "##2 D plot with dimensionality reduction\n",
        "\n",
        "def table_plot(reals,fakes,dimentionality_reduction='PCA',filename=''):\n",
        "\n",
        "\n",
        "  if dimentionality_reduction=='PCA':\n",
        "    model=KernelPCA(n_components=2,kernel=\"rbf\")#, whiten=True)\n",
        "  elif dimentionality_reduction=='TSNE': ##takes too long for big dataset\n",
        "    model=TSNE(n_components = 2, perplexity=10,random_state = 1)\n",
        "\n",
        "  realt=model.fit_transform(reals)\n",
        "  faket=model.fit_transform(fakes)\n",
        "\n",
        "  #fit the model to our data and extract the results\n",
        "  #create a dataframe from the dataset\n",
        "  real_pca = pd.DataFrame(data = realt ,columns = [\"Component 1\",\"Component 2\"])\n",
        "  fake_pca = pd.DataFrame(data = faket ,columns = [\"Component 1\",\"Component 2\"])\n",
        "\n",
        "  real_pca['dataset']='real'\n",
        "  fake_pca['dataset']='fake'\n",
        "\n",
        "  #plot the resulting data from two dimensions\n",
        "  g = sns.jointplot(data = pd.concat([real_pca,fake_pca]),\n",
        "                  x = \"Component 1\",\n",
        "                  y = \"Component 2\",\n",
        "                    palette=[\"#2171B5\",\"#6BAED6\"],\n",
        "                    # kind='kde',\n",
        "                    # fill=True,\n",
        "                    joint_kws={'alpha': 0.8},\n",
        "                    hue=\"dataset\")\n",
        "  g.fig.set_size_inches((5, 5))\n",
        "  if filename!='':\n",
        "    g.savefig(filename+'.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_k_l(real):\n",
        "\n",
        "  k_values = [999]\n",
        "  l_values = [999]\n",
        "  for n_clusters in np.arange(5,50,10):\n",
        "      if len(real) / n_clusters < 10:\n",
        "          continue\n",
        "      model = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=0).fit(real)\n",
        "      counts: dict = Counter(model.labels_)\n",
        "      k_values.append(np.min(list(counts.values())))\n",
        "\n",
        "      clusters = model.predict(real)\n",
        "      clusters_df = pd.Series(clusters, index=real.index)\n",
        "      for cluster in range(n_clusters):\n",
        "          partition = real[clusters_df == cluster]\n",
        "          uniq_values = partition.drop_duplicates()\n",
        "          l_values.append(len(uniq_values))\n",
        "\n",
        "\n",
        "  return [int(np.min(k_values)),int(np.min(l_values))]"
      ],
      "metadata": {
        "id": "5AX-0zEqDL7g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def small_counts_ratio(fake,c_col):\n",
        "  fake=fake.copy()\n",
        "  fake['counts'] = fake.groupby(c_col).cumcount() + 1\n",
        "  return sum(fake['counts']<6)/len(fake)"
      ],
      "metadata": {
        "id": "OjhntrOoHsf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_copies(real,fake,return_len: bool = True) -> Union[pd.DataFrame, int]:\n",
        "        \"\"\"\n",
        "        Check whether any real values occur in the fake data.\n",
        "        :param return_len: whether to return the length of the copied rows or not.\n",
        "        :return: Dataframe containing the duplicates if return_len=False, else integer indicating the number of copied rows.\n",
        "        \"\"\"\n",
        "        real_hashes = real.apply(lambda x: hash(tuple(x)), axis=1)\n",
        "        fake_hashes = fake.apply(lambda x: hash(tuple(x)), axis=1)\n",
        "\n",
        "        dup_idxs = fake_hashes.isin(real_hashes.values)\n",
        "        dup_idxs = dup_idxs[dup_idxs == True].sort_index().index.tolist()\n",
        "\n",
        "        copies = fake.loc[dup_idxs, :]\n",
        "\n",
        "        if return_len:\n",
        "            return len(copies)\n",
        "        else:\n",
        "            return copies"
      ],
      "metadata": {
        "id": "F0Ck07BWJPQN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvZzmxyvzAM5"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmVREdL6zSp_"
      },
      "outputs": [],
      "source": [
        "def uni_test(real,fake,c_col,n_col):\n",
        "\n",
        "  ks=num_statistics_df(real,fake,kolmogorov_smirnov_test,n_col)##equal distribution?\n",
        "  mw=num_statistics_df(real,fake,mannwhitneyu,n_col) #equal mean?\n",
        "  md=num_statistics_df(real,fake,mood_test,n_col)#equal median?\n",
        "  le=num_statistics_df(real,fake,levene_test,n_col) #equal variance?\n",
        "  cs=cat_statistics_df(real,fake,chisquare_test,c_col)#same frequency\n",
        "  kl=num_divergence_df(real,fake,kl_divergence)\n",
        "  js=num_divergence_df(real,fake,js_divergence)\n",
        "  em=num_distance_df(real,fake,em_distance)\n",
        "\n",
        "  result=pd.concat([ks,mw,md,le,cs,kl,js,em],axis=1)\n",
        "  return result[['ks_p-value','mw_p-value','md_p-value','le_p-value', 'ct_p-value', 'kl_divergence','js_distance', 'em_distance']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00G3aob8zB-n"
      },
      "outputs": [],
      "source": [
        "data_dir='/content/drive/MyDrive/Experiment/DATA/'\n",
        "dfs_path=['credit-g','national-longitudinal-survey-binary','company-bankruptcy','law-school-admission-bianry','adult','diabetes']\n",
        "folds=5\n",
        "GMs=['Simulation.csv','SMOTE.csv','ADASYN.csv','SVMSMOTE.csv','SMOTENC.csv','SMOTETomek.csv','Synthpop.csv','Copula.csv','ctgan.csv','tvae.csv']\n",
        "\n",
        "target_col=\"class\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-92Y6hvoiRG"
      },
      "source": [
        "###Statistical Similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for data_folder in dfs_path:\n",
        "  print(data_folder)\n",
        "  print(\"-\"*50)\n",
        "  uni_result={}\n",
        "  biv_result={}\n",
        "  mmd_result={}\n",
        "  pca_result={}\n",
        "  det_result={}\n",
        "  loaded=load_data(data_dir,data_folder,GMs)\n",
        "\n",
        "  c_col=loaded['c_col']\n",
        "  n_col=loaded['n_col']\n",
        "  real=loaded['real']\n",
        "\n",
        "  for syn_name,fake in loaded['fake'].items():\n",
        "    print(syn_name)\n",
        "    print(\"-\"*30)\n",
        "    fake.columns=real.columns\n",
        "    reals,fakes=data_preprocess(real,fake,c_col,onehot=False)\n",
        "    reale,fakee=data_preprocess(real,fake,c_col,onehot=True)\n",
        "    uni_result[syn_name]=uni_test(reals,fakes,c_col,n_col)\n",
        "\n",
        "    p,rc,fc=bivariate_test(reals,fakes,c_col)\n",
        "    biv_result[syn_name]=p\n",
        "\n",
        "    pca=pca_correlation(reals,fakes)\n",
        "    pca_result[syn_name]=pca\n",
        "\n",
        "    if reals.shape[0]>30000:\n",
        "      reals,fakes=reals.sample(30000),fakes.sample(30000)\n",
        "    mmd=mmd_kernel(reals,fakes)\n",
        "    mmd_result[syn_name]=mmd\n",
        "\n",
        "\n",
        "  pickle.dump(uni_result,open(data_dir+data_folder+'/uni_result.pickle', 'wb'))\n",
        "  pickle.dump(biv_result,open(data_dir+data_folder+'/biv_result.pickle', 'wb'))\n",
        "  pickle.dump(mmd_result,open(data_dir+data_folder+'/mmd_result.pickle', 'wb'))\n",
        "  pickle.dump(pca_result,open(data_dir+data_folder+'/pca_result.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "aPnvsVxNLmLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pri_test(real,fake,c_col):\n",
        "  fake[c_col]=fake[c_col].round()\n",
        "  dup=get_copies(real,fake)\n",
        "  sma=small_counts_ratio(fake,c_col)\n",
        "  return dup,sma"
      ],
      "metadata": {
        "id": "YHuf6X_dLEn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pri={}\n",
        "for data_folder in dfs_path:\n",
        "  print(data_folder)\n",
        "  print(\"-\"*50)\n",
        "  pri_result={}\n",
        "  loaded=load_data(data_dir,data_folder,GMs)\n",
        "\n",
        "  c_col=loaded['c_col']\n",
        "  n_col=loaded['n_col']\n",
        "  real=loaded['real']\n",
        "\n",
        "  for syn_name,fake in loaded['fake'].items():\n",
        "    print(syn_name)\n",
        "    print(\"-\"*30)\n",
        "    fake.columns=real.columns\n",
        "    pri=pri_test(real,fake,c_col)\n",
        "    pri_result[syn_name]=pri\n",
        "\n",
        "  # pickle.dump(det_result,open(data_dir+data_folder+'/det_result.pickle', 'wb'))\n",
        "  all_pri[data_folder]=pri_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7866-l1K2jD",
        "outputId": "a05267ad-d3c7-49f9-f511-47c1cce71d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "credit-customers\n",
            "--------------------------------------------------\n",
            "Simulation.csv\n",
            "SMOTE.csv\n",
            "SMOTENC.csv\n",
            "SMOTETomek.csv\n",
            "Synthpop.csv\n",
            "Copula.csv\n",
            "ctgan.csv\n",
            "tvae.csv\n",
            "SIMULATION\n",
            "------------------------------\n",
            "SMOTE\n",
            "------------------------------\n",
            "SMOTENC\n",
            "------------------------------\n",
            "SMOTETOMEK\n",
            "------------------------------\n",
            "SYNTHPOP\n",
            "------------------------------\n",
            "COPULA\n",
            "------------------------------\n",
            "CTGAN\n",
            "------------------------------\n",
            "TVAE\n",
            "------------------------------\n",
            "national-longitudinal-survey-binary\n",
            "--------------------------------------------------\n",
            "Simulation.csv\n",
            "SMOTE.csv\n",
            "SMOTENC.csv\n",
            "SMOTETomek.csv\n",
            "Synthpop.csv\n",
            "Copula.csv\n",
            "ctgan.csv\n",
            "tvae.csv\n",
            "SIMULATION\n",
            "------------------------------\n",
            "SMOTE\n",
            "------------------------------\n",
            "SMOTENC\n",
            "------------------------------\n",
            "SMOTETOMEK\n",
            "------------------------------\n",
            "SYNTHPOP\n",
            "------------------------------\n",
            "COPULA\n",
            "------------------------------\n",
            "CTGAN\n",
            "------------------------------\n",
            "TVAE\n",
            "------------------------------\n",
            "company-bankruptcy\n",
            "--------------------------------------------------\n",
            "Simulation.csv\n",
            "SMOTE.csv\n",
            "SMOTENC.csv\n",
            "SMOTETomek.csv\n",
            "Synthpop.csv\n",
            "Copula.csv\n",
            "ctgan.csv\n",
            "tvae.csv\n",
            "SIMULATION\n",
            "------------------------------\n",
            "SMOTE\n",
            "------------------------------\n",
            "SMOTENC\n",
            "------------------------------\n",
            "SMOTETOMEK\n",
            "------------------------------\n",
            "SYNTHPOP\n",
            "------------------------------\n",
            "COPULA\n",
            "------------------------------\n",
            "CTGAN\n",
            "------------------------------\n",
            "TVAE\n",
            "------------------------------\n",
            "law-school-admission-bianry\n",
            "--------------------------------------------------\n",
            "Simulation.csv\n",
            "SMOTE.csv\n",
            "SMOTENC.csv\n",
            "SMOTETomek.csv\n",
            "Synthpop.csv\n",
            "Copula.csv\n",
            "ctgan.csv\n",
            "tvae.csv\n",
            "SIMULATION\n",
            "------------------------------\n",
            "SMOTE\n",
            "------------------------------\n",
            "SMOTENC\n",
            "------------------------------\n",
            "SMOTETOMEK\n",
            "------------------------------\n",
            "SYNTHPOP\n",
            "------------------------------\n",
            "COPULA\n",
            "------------------------------\n",
            "CTGAN\n",
            "------------------------------\n",
            "TVAE\n",
            "------------------------------\n",
            "diabetes\n",
            "--------------------------------------------------\n",
            "Simulation.csv\n",
            "SMOTE.csv\n",
            "SMOTENC.csv\n",
            "SMOTETomek.csv\n",
            "Synthpop.csv\n",
            "Copula.csv\n",
            "ctgan.csv\n",
            "tvae.csv\n",
            "SIMULATION\n",
            "------------------------------\n",
            "SMOTE\n",
            "------------------------------\n",
            "SMOTENC\n",
            "------------------------------\n",
            "SMOTETOMEK\n",
            "------------------------------\n",
            "SYNTHPOP\n",
            "------------------------------\n",
            "COPULA\n",
            "------------------------------\n",
            "CTGAN\n",
            "------------------------------\n",
            "TVAE\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t=[]\n",
        "for dn,df in all_pri.items():\n",
        "  t.append(pd.DataFrame(all_pri[dn]).T)"
      ],
      "metadata": {
        "id": "7SI6xhv4OXla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoK39gz6onOj"
      },
      "source": [
        "###ML utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nr9yPSAqyH42"
      },
      "outputs": [],
      "source": [
        "data_dir='/content/drive/MyDrive/Experiment/DATA/'\n",
        "\n",
        "folds=5\n",
        "\n",
        "for data_folder in dfs_path:\n",
        "  target_col=\"class\"\n",
        "  if data_folder in ['national-longitudinal-survey-binary','law-school-admission-bianry']:\n",
        "    target_col=\"label\"\n",
        "  df_result={}\n",
        "  print(data_folder)\n",
        "  for i in range(folds):\n",
        "    print(f'Fold {i} is currently processing------------------------')\n",
        "    print('\\n')\n",
        "    seed_folder=data_dir+data_folder+'/synthetic/seed'+str(i)\n",
        "    baseline=pd.read_csv(os.path.join(seed_folder, 'baseline.csv'))\n",
        "    df_test=pd.read_csv(os.path.join(seed_folder, 'df_test.csv'))\n",
        "    r=ml_evaluation(baseline,baseline,df_test,target_col)\n",
        "    df_result[data_folder+'.baseline'+'.seed'+str(i)]=r\n",
        "    for file in GMs:\n",
        "      print(file)\n",
        "      fake=pd.read_csv(os.path.join(seed_folder, file))\n",
        "      fake.columns=baseline.columns\n",
        "      fake[target_col]=adjusted_classes(fake[target_col],0.5)\n",
        "      r=ml_evaluation(baseline,fake,df_test,target_col)\n",
        "      df_result[data_folder+'.'+file[:-4]+'.seed'+str(i)]=r\n",
        "  pickle.dump(df_result,open(data_dir+data_folder+'/ml_result.pickle', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdJtCSq_d5-A",
        "outputId": "651acf45-fbce-4271-e988-b445d1849379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "credit-customers\n",
            "--------------------------------------------------\n",
            "national-longitudinal-survey-binary\n",
            "--------------------------------------------------\n",
            "company-bankruptcy\n",
            "--------------------------------------------------\n",
            "law-school-admission-bianry\n",
            "--------------------------------------------------\n",
            "diabetes\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "all_uni={}\n",
        "all_biv={}\n",
        "all_mmd={}\n",
        "all_pca={}\n",
        "all_ml={}\n",
        "all_det={}\n",
        "for data_folder in dfs_path:\n",
        "  temp={}\n",
        "  print(data_folder)\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  uni_data = pickle.load(open(data_dir+data_folder+'/uni_result.pickle', 'rb'))\n",
        "  for key, df in uni_data.items():\n",
        "    temp[key]=df.loc['mean']\n",
        "  all_uni[data_folder]=pd.DataFrame(temp).T\n",
        "\n",
        "  biv_data = pickle.load(open(data_dir+data_folder+'/biv_result.pickle', 'rb'))\n",
        "  all_biv[data_folder]=biv_data\n",
        "\n",
        "  mmd_data = pickle.load(open(data_dir+data_folder+'/mmd_result.pickle', 'rb'))\n",
        "  all_mmd[data_folder]=mmd_data\n",
        "\n",
        "  pca_data = pickle.load(open(data_dir+data_folder+'/pca_result.pickle', 'rb'))\n",
        "  all_pca[data_folder]=pca_data\n",
        "\n",
        "  ml_data = pickle.load(open(data_dir+data_folder+'/ml_result.pickle', 'rb'))\n",
        "  all_ml[data_folder]=pd.concat(ml_data)\n",
        "\n",
        "  det_data = pickle.load(open(data_dir+data_folder+'/det_result.pickle', 'rb'))\n",
        "  all_det[data_folder]=pd.concat(det_data)\n",
        "\n",
        "\n",
        "# pd.concat(all_uni).to_csv('uni.test.csv')\n",
        "# pd.DataFrame(all_biv).T.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_uni={}\n",
        "all_biv={}\n",
        "all_mmd={}\n",
        "all_pca={}\n",
        "all_ml={}\n",
        "all_dis={}\n",
        "for data_folder in dfs_path:\n",
        "  temp={}\n",
        "  print(data_folder)\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  uni_data = pickle.load(open(data_dir+data_folder+'/uni_result.pickle', 'rb'))\n",
        "  for key, df in uni_data.items():\n",
        "    temp[key]=df.loc['mean']\n",
        "  all_uni[data_folder]=pd.DataFrame(temp).T\n",
        "\n",
        "  biv_data = pickle.load(open(data_dir+data_folder+'/biv_result.pickle', 'rb'))\n",
        "  all_biv[data_folder]=biv_data\n",
        "\n",
        "  mmd_data = pickle.load(open(data_dir+data_folder+'/mmd_result.pickle', 'rb'))\n",
        "  all_mmd[data_folder]=mmd_data\n",
        "\n",
        "  pca_data = pickle.load(open(data_dir+data_folder+'/pca_result.pickle', 'rb'))\n",
        "  all_pca[data_folder]=pca_data\n",
        "\n",
        "  ml_data = pickle.load(open(data_dir+data_folder+'/ml_result.pickle', 'rb'))\n",
        "  all_ml[data_folder]=pd.concat(ml_data)\n",
        "\n",
        "  dis_data = pickle.load(open(data_dir+data_folder+'/dis_result.pickle', 'rb'))\n",
        "  all_dis[data_folder]=pd.DataFrame(dis_data).T\n",
        "\n",
        "# pd.concat(all_uni).to_csv('uni.test.csv')\n",
        "# pd.DataFrame(all_biv).T.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N22AGuK_ukJd",
        "outputId": "1e35aa8b-e2fd-4cab-959c-1cfe51c536c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "credit-customers\n",
            "--------------------------------------------------\n",
            "national-longitudinal-survey-binary\n",
            "--------------------------------------------------\n",
            "company-bankruptcy\n",
            "--------------------------------------------------\n",
            "law-school-admission-bianry\n",
            "--------------------------------------------------\n",
            "diabetes\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(dis_data).T[['distance_mean','distance_1nn']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "f_E4CedbvmJf",
        "outputId": "f03ce89c-2196-425c-f5fe-0c71550497bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            distance_mean  distance_mean  distance_1nn\n",
              "SIMULATION           3.67           0.33          1.76\n",
              "SMOTE                3.59           0.32          1.25\n",
              "SMOTENC              3.56           0.35          1.27\n",
              "SMOTETOMEK           3.59           0.32          1.25\n",
              "SYNTHPOP             3.69           0.41          2.12\n",
              "COPULA               3.87           0.44          2.22\n",
              "CTGAN                3.69           0.38          1.65\n",
              "TVAE                 3.61           0.36          1.59"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-34a8a982-07e9-450b-b3b6-613bb748bec5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>distance_mean</th>\n",
              "      <th>distance_mean</th>\n",
              "      <th>distance_1nn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SIMULATION</th>\n",
              "      <td>3.67</td>\n",
              "      <td>0.33</td>\n",
              "      <td>1.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SMOTE</th>\n",
              "      <td>3.59</td>\n",
              "      <td>0.32</td>\n",
              "      <td>1.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SMOTENC</th>\n",
              "      <td>3.56</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SMOTETOMEK</th>\n",
              "      <td>3.59</td>\n",
              "      <td>0.32</td>\n",
              "      <td>1.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SYNTHPOP</th>\n",
              "      <td>3.69</td>\n",
              "      <td>0.41</td>\n",
              "      <td>2.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>COPULA</th>\n",
              "      <td>3.87</td>\n",
              "      <td>0.44</td>\n",
              "      <td>2.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CTGAN</th>\n",
              "      <td>3.69</td>\n",
              "      <td>0.38</td>\n",
              "      <td>1.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TVAE</th>\n",
              "      <td>3.61</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1.59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34a8a982-07e9-450b-b3b6-613bb748bec5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-0eb2040b-a84f-4b08-b327-f597f7ec523a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0eb2040b-a84f-4b08-b327-f597f7ec523a')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-0eb2040b-a84f-4b08-b327-f597f7ec523a button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34a8a982-07e9-450b-b3b6-613bb748bec5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34a8a982-07e9-450b-b3b6-613bb748bec5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gW31nfYrngcu",
        "PMcL_ARpCb1d",
        "Jai97Ok-WCx2",
        "10SRpPsqt89Y",
        "hWRzdIPU1vKU",
        "9KqPs-3511q2",
        "X1uAZB1f09wC",
        "g3fQpS-_4iO6"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}